{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9228b-e2b6-45fb-ab69-801cd4e64928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "from som import Som, cluster\n",
    "\n",
    "def calc_anom(da):\n",
    "    climatology_mean = da.groupby(\"time.month\").mean(\"time\")\n",
    "    climatology_std = da.groupby(\"time.month\").std(\"time\")\n",
    "    anom = xr.apply_ufunc(\n",
    "        lambda x, m, s: (x - m) / s,\n",
    "        da.groupby(\"time.month\"),\n",
    "        climatology_mean,\n",
    "        climatology_std,\n",
    "    )\n",
    "    return anom\n",
    "\n",
    "print(\"Self-Organizing Feature Map Analysis\")\n",
    "# Load data\n",
    "ds = xr.open_dataset(\"../Data/u850.nc\")\n",
    "uwnd850 = ds[\"uwnd\"].sel(\n",
    "    time=slice(\"1991\", \"2023\"),\n",
    "    lat=slice(60.0, -10.0),\n",
    "    lon=slice(100.0, 180.0),\n",
    ")\n",
    "uwnd850 = uwnd850.sel(time=uwnd850.time.dt.month.isin([6, 7, 8]))\n",
    "uwnd850_anom = calc_anom(uwnd850)\n",
    "uwnd850_anom = uwnd850_anom.drop_vars([\"month\", \"level\"])\n",
    "# Coordinates\n",
    "lon = uwnd850_anom.lon\n",
    "lat = uwnd850_anom.lat\n",
    "nd = len(lon) * len(lat)\n",
    "nt = uwnd850_anom.time.size\n",
    "\n",
    "# Reshape to (nd, nt)\n",
    "x = uwnd850_anom.values.reshape(nt, nd).T\n",
    "\n",
    "# SOM parameters\n",
    "m1 = 3\n",
    "m2 = 3\n",
    "nter = 5000\n",
    "alpha0 = 5.0\n",
    "alphamin = 0.05\n",
    "taua = 1000\n",
    "sigma0 = 2.5\n",
    "taus = 500\n",
    "\n",
    "# Train SOM\n",
    "w = Som(x, nd, nt, m1, m2, nter, alpha0, taua, alphamin, sigma0, taus)\n",
    "\n",
    "# Cluster\n",
    "cluster1, cluster2, qerr, qerror, count = cluster(x, w, nd, nt, m1, m2)\n",
    "\n",
    "# ---------------------------\n",
    "# Average maps per neuron\n",
    "# ---------------------------\n",
    "# Pre-allocate storage for average maps (lon x lat x neurons)\n",
    "avg_maps = np.empty((len(lon), len(lat), m1 * m2), dtype=float)\n",
    "\n",
    "# Build neuron indices for each sample (1-based like Julia)\n",
    "neuron_indices = cluster1 + (cluster2 - 1) * m1  # shape (nt,)\n",
    "\n",
    "for neuron in range(1, m1 * m2 + 1):\n",
    "    inds = np.where(neuron_indices == neuron)[0]\n",
    "    if inds.size == 0:\n",
    "        avg_maps[:, :, neuron - 1] = np.nan\n",
    "    else:\n",
    "        # mean over selected time indices\n",
    "        sel = uwnd850_anom.isel(time=inds).mean(dim=\"time\", keep_attrs=False)\n",
    "        sel = sel.transpose(\"lon\", \"lat\")\n",
    "        avg_maps[:, :, neuron - 1] = sel.values\n",
    "\n",
    "vmin = -0.05\n",
    "vmax = 0.05\n",
    "boundaries = np.linspace(-0.05, 0.05, 11)\n",
    "norm = BoundaryNorm(boundaries=boundaries, ncolors=256, extend=\"both\")\n",
    "\n",
    "fig = plt.figure(layout=\"constrained\", figsize=(m1 * 5, m2 * 4))\n",
    "axes = fig.subplots(3, 3, subplot_kw={\"projection\": ccrs.PlateCarree()})\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    feature = avg_maps[:, :, i]\n",
    "    im = ax.contourf(\n",
    "        uwnd850_anom.lon,\n",
    "        uwnd850_anom.lat,\n",
    "        feature.T,\n",
    "        cmap=\"RdBu_r\",\n",
    "        norm=norm,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "    ax.contour(im, colors=\"black\", linewidths=(0.5,))\n",
    "    ax.set_title(f\"Cluster {i + 1}: {count[i]}\", fontsize=14)\n",
    "    ax.coastlines()\n",
    "    gl = ax.gridlines(draw_labels=True)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    ax.set_extent([100, 180, -10, 60], crs=ccrs.PlateCarree())\n",
    "\n",
    "cbar = fig.colorbar(\n",
    "    ScalarMappable(norm=norm, cmap=\"RdBu_r\"),\n",
    "    ax=axes[:, :],\n",
    "    location=\"bottom\",\n",
    "    fraction=0.05,\n",
    "    pad=0.02,\n",
    ")\n",
    "cbar.set_label(\"U-wind 850hPa Anomaly (m/s)\", fontsize=12)\n",
    "plt.suptitle(\"U-wind 850hPa Anomaly SOM\", fontsize=16)\n",
    "plt.savefig(\"../Results/uwnd850_anom_som.png\", dpi=300)\n",
    "\n",
    "# Counting\n",
    "clusters = cluster1 + (cluster2 - 1) * 3\n",
    "\n",
    "time = uwnd850_anom.time\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\"cluster\": clusters},\n",
    "    index=pd.to_datetime(time),\n",
    ")\n",
    "\n",
    "df[\"cluster\"] = df[\"cluster\"].astype(\"category\")\n",
    "\n",
    "# Extract year\n",
    "df[\"year\"] = df.index.to_period(freq=\"Y\")\n",
    "\n",
    "# Group and count\n",
    "counts = df.groupby([\"cluster\", \"year\"], observed=True).size().reset_index(name=\"count\")\n",
    "counts = counts.pivot(index=\"year\", columns=\"cluster\", values=\"count\").fillna(0)\n",
    "counts.to_csv(\"../Data/u850_cluster_counts.csv\")\n",
    "\n",
    "# Plot time series\n",
    "# Create one figure with 3x3 subplots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12), layout=\"constrained\")\n",
    "axes = axes.flatten()  # flatten to easily iterate over\n",
    "t = range(1991, 2024)\n",
    "for i in range(0, 9):\n",
    "    ax = axes[i]\n",
    "    ax.plot(t, counts[i + 1], \"-o\")\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    ax.set_ylabel(\"Counts\")\n",
    "    ax.set_ylim(-5, 50)\n",
    "    ax.set_xlim(1990, 2025)\n",
    "    ax.tick_params(axis=\"both\", tickdir=\"in\")\n",
    "    ax.grid(True)\n",
    "    ax.set_title(f\"cluster {i + 1}: {int(counts[i + 1].sum())}\")\n",
    "\n",
    "fig.suptitle(\"Occurences each year\")\n",
    "# Save one combined figure\n",
    "plt.savefig(\"../Results/u850a_cluster_timeseries.png\")\n",
    "\n",
    "# u850\n",
    "# ds = xr.open_dataset(\"../Data/u850.nc\")\n",
    "ds = ds.sel(time=ds.time.dt.month.isin([6, 7, 8]))\n",
    "ds.drop_vars(\"level\")\n",
    "print(ds)\n",
    "\n",
    "ds_monthly = ds.resample(time=\"ME\").mean()\n",
    "\n",
    "\n",
    "clima = ds_monthly.groupby(\"time.month\").mean(\"time\")\n",
    "\n",
    "clima\n",
    "\n",
    "anom = ds_monthly.groupby(\"time.month\") - clima\n",
    "print(anom)\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from scipy.stats import t\n",
    "\n",
    "# ---------------------------\n",
    "# Composite for ANOM_JJA with Significance Test\n",
    "# ---------------------------\n",
    "\n",
    "# Ensure data is filtered to JJA and aligned with SOM samples\n",
    "da_comp = anom  # Assuming 'anom' is your anomaly DataArray\n",
    "if 'month' in da_comp.coords:\n",
    "    da_comp = da_comp.drop_vars('month')\n",
    "\n",
    "# Ensure JJA selection\n",
    "da_comp_jja = da_comp.sel(time=da_comp.time.dt.month.isin([6, 7, 8]))\n",
    "\n",
    "# Building neuron labels (1-based) from SOM output\n",
    "# m1, m2, cluster1, cluster2 come from the SOM training section\n",
    "neuron_labels = cluster1 + (cluster2 - 1) * m1\n",
    "\n",
    "# Pre-allocate storage\n",
    "# avg_maps: Stores the mean anomaly\n",
    "# sig_maps: Stores the p-value or significance mask\n",
    "n_lon = len(da_comp_jja.lon)\n",
    "n_lat = len(da_comp_jja.lat)\n",
    "n_neurons = m1 * m2\n",
    "\n",
    "avg_maps_comp = np.empty((n_lon, n_lat, n_neurons), dtype=float)\n",
    "sig_maps_comp = np.empty((n_lon, n_lat, n_neurons), dtype=float)\n",
    "\n",
    "print(\"Calculating composites and significance...\")\n",
    "\n",
    "for neuron in range(1, n_neurons + 1):\n",
    "    inds = np.where(neuron_labels == neuron)[0]\n",
    "    \n",
    "    if inds.size < 2: # Need at least 2 samples for variance\n",
    "        avg_maps_comp[:, :, neuron - 1] = np.nan\n",
    "        sig_maps_comp[:, :, neuron - 1] = np.nan\n",
    "    else:\n",
    "        # Select the subset of data for this cluster\n",
    "        subset = da_comp_jja.isel(time=inds)\n",
    "        \n",
    "        # 1. Calculate Mean\n",
    "        # keep_attrs=False prevents xarray from dropping coords sometimes\n",
    "        clus_mean = subset.mean(dim=\"time\", skipna=True)\n",
    "        \n",
    "        # 2. Calculate Std Dev (ddof=1 for sample std)\n",
    "        clus_std = subset.std(dim=\"time\", ddof=1, skipna=True)\n",
    "        \n",
    "        # 3. Calculate N (sample size)\n",
    "        # If there are NaNs in space, count valid observations per grid point\n",
    "        n_samples = subset.count(dim=\"time\") \n",
    "        \n",
    "        # 4. T-statistic: t = (mean - 0) / (std / sqrt(n))\n",
    "        # We assume population mean of anomalies is 0\n",
    "        se = clus_std / np.sqrt(n_samples)\n",
    "        t_stat = clus_mean / se\n",
    "        \n",
    "        # 5. Two-tailed P-value\n",
    "        # degrees of freedom = n - 1\n",
    "        dof = n_samples - 1\n",
    "        pval = 2 * t.sf(np.abs(t_stat), dof)\n",
    "        \n",
    "        # Transpose to (lon, lat)\n",
    "        if 'lon' in clus_mean.dims and 'lat' in clus_mean.dims:\n",
    "            clus_mean = clus_mean.transpose(\"lon\", \"lat\")\n",
    "            pval = pval.transpose(\"lon\", \"lat\") # pval is an xarray object here\n",
    "            \n",
    "        avg_maps_comp[:, :, neuron - 1] = clus_mean.values\n",
    "        sig_maps_comp[:, :, neuron - 1] = pval.values\n",
    "\n",
    "# Update global variables\n",
    "avg_maps = avg_maps_comp\n",
    "# We will use this for plotting (values < 0.05 are significant)\n",
    "p_values_maps = sig_maps_comp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization with Stippling\n",
    "# ---------------------------\n",
    "\n",
    "# Plot parameters\n",
    "vmin = -0.5 # Adjust based on your data range\n",
    "vmax = 0.5\n",
    "boundaries = np.linspace(vmin, vmax, 21)\n",
    "norm = BoundaryNorm(boundaries=boundaries, ncolors=256, extend=\"both\")\n",
    "\n",
    "# Significance Level (e.g., 95%)\n",
    "sig_level = 0.05 \n",
    "\n",
    "fig = plt.figure(layout=\"constrained\", figsize=(m1 * 5, m2 * 4))\n",
    "axes = fig.subplots(\n",
    "    m2, m1, subplot_kw={\"projection\": ccrs.PlateCarree(central_longitude=180)}\n",
    ")\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Data for this cluster\n",
    "    feature = avg_maps[:, :, i]\n",
    "    pval = p_values_maps[:, :, i]\n",
    "    \n",
    "    # 1. Plot the color shading (Mean Anomaly)\n",
    "    im = ax.contourf(\n",
    "        da_comp_jja.lon,\n",
    "        da_comp_jja.lat,\n",
    "        feature.T, # Transpose if necessary based on your array shape\n",
    "        cmap=\"RdBu_r\",\n",
    "        norm=norm,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "    )\n",
    "    \n",
    "    # 2. Add Contours for clarity\n",
    "    # ax.contour(im, colors=\"k\", linewidths=(0.5,), transform=ccrs.PlateCarree())\n",
    "    \n",
    "    # 3. Add Stippling for Significance\n",
    "    # We mask values where p-value > 0.05 (insignificant) so they are NOT hatched\n",
    "    # or we plot specific levels.\n",
    "    \n",
    "    # Method: Contourf with hatches\n",
    "    # levels: [0, sig_level, 1] -> \n",
    "    # Bin 1: 0 to 0.05 (Significant) -> Apply Hatch\n",
    "    # Bin 2: 0.05 to 1 (Not Significant) -> No Hatch\n",
    "    ax.contourf(\n",
    "        da_comp_jja.lon,\n",
    "        da_comp_jja.lat,\n",
    "        pval.T,\n",
    "        levels=[0, sig_level, 1],\n",
    "        hatches=['...', ''], # '...' = dots, '' = nothing\n",
    "        colors='none', # Make the fill transparent, only show hatches\n",
    "        transform=ccrs.PlateCarree()\n",
    "    )\n",
    "\n",
    "    # Titles and formatting\n",
    "    ax.set_title(f\"Cluster {i + 1} (n={count[i]})\", fontsize=14)\n",
    "    ax.coastlines()\n",
    "    \n",
    "    # Extent\n",
    "    ax.set_extent([0, 180, -60, 60], crs=ccrs.PlateCarree())\n",
    "    \n",
    "    # Gridlines\n",
    "    gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.5)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "\n",
    "# Colorbar\n",
    "cbar = fig.colorbar(\n",
    "    ScalarMappable(norm=norm, cmap=\"RdBu_r\"),\n",
    "    ax=axes[:, :],\n",
    "    location=\"bottom\",\n",
    "    fraction=0.05,\n",
    "    pad=0.02,\n",
    ")\n",
    "cbar.set_label(\"U-wind 850hPa Anomaly (m/s) [Stippling: p < 0.05]\", fontsize=12)\n",
    "plt.suptitle(\"U-wind 850hPa Anomaly Composite (JJA)\", fontsize=16)\n",
    "\n",
    "save_path = \"../Results/uwnd850_anom_som_significant.png\"\n",
    "plt.savefig(save_path, dpi=300)\n",
    "print(f\"Figure saved to {save_path}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
